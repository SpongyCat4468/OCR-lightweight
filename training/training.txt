Epoch 1/100 - Loss: 1.8960 - LR: 0.000500
Epoch 2/100 - Loss: 0.6230 - LR: 0.000500
Epoch 3/100 - Loss: 0.4750 - LR: 0.000500
Epoch 4/100 - Loss: 0.3917 - LR: 0.000500
Epoch 5/100 - Loss: 0.3340 - LR: 0.000500
Epoch 6/100 - Loss: 0.2885 - LR: 0.000500
Epoch 7/100 - Loss: 0.2570 - LR: 0.000500
Epoch 8/100 - Loss: 0.2382 - LR: 0.000500
Epoch 9/100 - Loss: 0.2269 - LR: 0.000500
Epoch 10/100 - Loss: 0.2159 - LR: 0.000500
Epoch 11/100 - Loss: 0.2001 - LR: 0.000500
Epoch 12/100 - Loss: 0.1876 - LR: 0.000500
Epoch 13/100 - Loss: 0.1715 - LR: 0.000500
Epoch 14/100 - Loss: 0.1710 - LR: 0.000500
Epoch 15/100 - Loss: 0.1560 - LR: 0.000500
Epoch 16/100 - Loss: 0.1649 - LR: 0.000500
Epoch 17/100 - Loss: 0.1521 - LR: 0.000500
Epoch 18/100 - Loss: 0.1529 - LR: 0.000500
Epoch 19/100 - Loss: 0.1417 - LR: 0.000500
Epoch 20/100 - Loss: 0.1381 - LR: 0.000500
Epoch 21/100 - Loss: 0.1401 - LR: 0.000500
Epoch 22/100 - Loss: 0.1321 - LR: 0.000500
Epoch 23/100 - Loss: 0.1336 - LR: 0.000500
Epoch 24/100 - Loss: 0.1564 - LR: 0.000500
Epoch 25/100 - Loss: 0.1411 - LR: 0.000500
Epoch 26/100 - Loss: 0.1377 - LR: 0.000500
Epoch 27/100 - Loss: 0.1358 - LR: 0.000500
Epoch 28/100 - Loss: 0.1294 - LR: 0.000500
Epoch 29/100 - Loss: 0.1333 - LR: 0.000500
Epoch 30/100 - Loss: 0.1171 - LR: 0.000500
Epoch 31/100 - Loss: 0.1239 - LR: 0.000500
Epoch 32/100 - Loss: 0.1208 - LR: 0.000500
Epoch 33/100 - Loss: 0.1085 - LR: 0.000500
Epoch 34/100 - Loss: 0.1019 - LR: 0.000500
Epoch 35/100 - Loss: 0.1018 - LR: 0.000500
Epoch 36/100 - Loss: 0.0978 - LR: 0.000500
Epoch 37/100 - Loss: 0.0992 - LR: 0.000500
Epoch 38/100 - Loss: 0.1040 - LR: 0.000500
Epoch 39/100 - Loss: 0.1000 - LR: 0.000500
Epoch 40/100 - Loss: 0.1021 - LR: 0.000500
Epoch 41/100 - Loss: 0.1035 - LR: 0.000500
Epoch 42/100 - Loss: 0.1007 - LR: 0.000500
Epoch 43/100 - Loss: 0.1014 - LR: 0.000500
Epoch 44/100 - Loss: 0.0946 - LR: 0.000500
Epoch 45/100 - Loss: 0.0835 - LR: 0.000500
Epoch 46/100 - Loss: 0.0878 - LR: 0.000500
Epoch 47/100 - Loss: 0.0783 - LR: 0.000500
Epoch 48/100 - Loss: 0.0857 - LR: 0.000500
Epoch 49/100 - Loss: 0.0905 - LR: 0.000500
Epoch 50/100 - Loss: 0.0994 - LR: 0.000500
Epoch 51/100 - Loss: 0.0876 - LR: 0.000500
Epoch 52/100 - Loss: 0.0767 - LR: 0.000500
Epoch 53/100 - Loss: 0.0879 - LR: 0.000500
Epoch 54/100 - Loss: 0.0955 - LR: 0.000500
Epoch 55/100 - Loss: 0.0869 - LR: 0.000500
Epoch 56/100 - Loss: 0.0805 - LR: 0.000500
Epoch 57/100 - Loss: 0.0702 - LR: 0.000500
Epoch 58/100 - Loss: 0.0742 - LR: 0.000500
Epoch 59/100 - Loss: 0.0708 - LR: 0.000500
Epoch 60/100 - Loss: 0.0762 - LR: 0.000500
Epoch 61/100 - Loss: 0.0681 - LR: 0.000500
Epoch 62/100 - Loss: 0.0803 - LR: 0.000500
Epoch 63/100 - Loss: 0.0737 - LR: 0.000500
Epoch 64/100 - Loss: 0.0721 - LR: 0.000500
Epoch 65/100 - Loss: 0.0681 - LR: 0.000500
Epoch 66/100 - Loss: 0.0633 - LR: 0.000500
Epoch 67/100 - Loss: 0.0596 - LR: 0.000500
Epoch 68/100 - Loss: 0.0588 - LR: 0.000500
Epoch 69/100 - Loss: 0.0591 - LR: 0.000500
Epoch 70/100 - Loss: 0.0566 - LR: 0.000500
Epoch 71/100 - Loss: 0.0532 - LR: 0.000500
Epoch 72/100 - Loss: 0.0530 - LR: 0.000500
Epoch 73/100 - Loss: 0.0547 - LR: 0.000500
Epoch 74/100 - Loss: 0.0604 - LR: 0.000500
Epoch 75/100 - Loss: 0.0582 - LR: 0.000500
Epoch 76/100 - Loss: 0.0557 - LR: 0.000500
Epoch 77/100 - Loss: 0.0502 - LR: 0.000500
Epoch 78/100 - Loss: 0.0507 - LR: 0.000500
Epoch 79/100 - Loss: 0.0524 - LR: 0.000500
Epoch 80/100 - Loss: 0.0498 - LR: 0.000500
Epoch 81/100 - Loss: 0.0536 - LR: 0.000500
Epoch 82/100 - Loss: 0.0564 - LR: 0.000500
Epoch 83/100 - Loss: 0.0552 - LR: 0.000500
Epoch 84/100 - Loss: 0.0499 - LR: 0.000500
Epoch 85/100 - Loss: 0.0496 - LR: 0.000500
Epoch 86/100 - Loss: 0.0518 - LR: 0.000500
Epoch 87/100 - Loss: 0.0428 - LR: 0.000500
Epoch 88/100 - Loss: 0.0462 - LR: 0.000500
Epoch 89/100 - Loss: 0.0493 - LR: 0.000500
Epoch 90/100 - Loss: 0.0443 - LR: 0.000500
Epoch 91/100 - Loss: 0.0466 - LR: 0.000500
Epoch 92/100 - Loss: 0.0401 - LR: 0.000500
Epoch 93/100 - Loss: 0.0423 - LR: 0.000500
Epoch 94/100 - Loss: 0.0436 - LR: 0.000500
Epoch 95/100 - Loss: 0.0421 - LR: 0.000500
Epoch 96/100 - Loss: 0.0447 - LR: 0.000500
Epoch 97/100 - Loss: 0.0394 - LR: 0.000500
Epoch 98/100 - Loss: 0.0412 - LR: 0.000500
Epoch 99/100 - Loss: 0.0481 - LR: 0.000500
Epoch 100/100 - Loss: 0.0443 - LR: 0.000500

----- base -----


Step 1: Secure the "Gold Standard" AlphabetBefore touching the new data, you must ensure your model's "brain" doesn't get confused 
by shifting indices. Youâ€™ve already modified your CharsetMapper, so now you just need to ensure the .json file is generated from 
your SynthText training.Action: Run a small script to save your SynthText alphabet.Why: If SynthText had 63 characters and IIIT-5K
only has 36, a dynamic mapper would change the index of 'a' from 38 to 11, making your pre-trained weights useless.

Step 2: Extract
and Verify the "MAT" LabelsIIIT-5K doesn't use simple .txt files; it uses MATLAB .mat files. These are essentially dictionaries 
of arrays.Action: Use scipy.io.loadmat to peek inside traindata.mat.The Structure: Inside the traindata key, you'll find a 
nested list where item[0] is the image path (e.g., train/1_1.png) and item[1] is the text (e.g., WORLD).Pre-processing Trick: 
Many real-world labels are case-sensitive. If your SynthText model was trained on uppercase only, you must add .upper() to the 
labels as you load them.

Step 3: Implement the "Real-World" ResizeReal-world images have messy aspect ratios. If you simply 
squash a long word into a $128 \times 32$ box, it will look like barcodes to the model.The Strategy: Resize the height to 
exactly 32px and calculate the width to maintain the aspect ratio. Then, either:Pad the right side with black pixels to 
reach your model's expected width (e.g., 128px or 256px).Resize to a fixed width if your model was trained that way 
(though padding is generally more accurate for OCR).

Step 4: The "Surgical" Fine-Tuning SetupYou aren't just "training" 
anymore; you are "fine-tuning." This requires a gentler approach.Freeze the CNN: The CNN (feature extractor) already 
knows what an "S" or a "5" looks like. Lock those layers for the first 5 epochs so only the RNN (sequence logic) learns 
the new real-world textures.Learning Rate: Drop it from 0.0005 to 0.00005. High learning rates at this stage will cause 
"Catastrophic Forgetting," where the model forgets how to read synthetic text before it learns to read real text.

Step 5: 
The Validation/Test SplitAs we discussed, IIIT-5K gives you 2,000 training images and 3,000 test images.Action: Take 200 
images out of that training set to use as your "Validation" set.Why: You need a way to tell if the model is actually getting 
better or just memorizing the small IIIT-5K training pool.Epoch 2/5 - Loss: 10.0963 - LR: 0.000050
Epoch 3/5 - Loss: 7.3081 - LR: 0.000050
Epoch 4/5 - Loss: 5.4957 - LR: 0.000050
Epoch 5/5 - Loss: 4.0830 - LR: 0.000050
